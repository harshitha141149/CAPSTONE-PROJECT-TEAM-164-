# -*- coding: utf-8 -*-
"""CAPSTONE PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16-iqZ-BPTpBj5g1PFBwBQ7dBLApEU-Cy
"""

import warnings
warnings.filterwarnings("ignore")

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, StackingClassifier, IsolationForest
from sklearn.svm import OneClassSVM

# imbalanced-learn
from imblearn.over_sampling import SMOTE

# xgboost / lightgbm
from xgboost import XGBClassifier
import lightgbm as lgb

# keras autoencoder
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.callbacks import EarlyStopping

# shap
import shap
import joblib

# ---------- CONFIG ----------
DATA_PATH = "credit_card_fraud_dataset.csv"
RANDOM_STATE = 42

# ---------- UTILITY FUNCTIONS ----------

def find_label_column(df):
    candidates = ['Class', 'class', 'fraud', 'isFraud', 'Fraud', 'label', 'target', 'Y']
    for c in candidates:
        if c in df.columns:
            return c
    last_col = df.columns[-1]
    if set(df[last_col].dropna().unique()).issubset({0, 1}):
        return last_col
    raise KeyError("Could not determine label column automatically.")

def preprocess_features(X):
    """Handle non-numeric and datetime columns."""
    non_numeric = X.select_dtypes(include=['object']).columns
    print("Non-numeric columns detected:", non_numeric.tolist())

    for col in non_numeric:
        try:
            X[col] = pd.to_datetime(X[col], errors='coerce')
            X[col + '_hour'] = X[col].dt.hour.fillna(0)
            X[col + '_day'] = X[col].dt.day.fillna(0)
            X[col + '_weekday'] = X[col].dt.weekday.fillna(0)
            X.drop(columns=[col], inplace=True)
        except Exception:
            X = pd.get_dummies(X, columns=[col], drop_first=True)

    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)
    return X

def evaluate_model(name, model, X_test, y_test, proba=True):
    """Print classification report and ROC/PR AUC metrics."""
    if proba and hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test)[:, 1]
    elif proba and hasattr(model, "decision_function"):
        y_score = model.decision_function(X_test)
    else:
        y_score = model.predict(X_test)
    y_pred = (y_score >= 0.5).astype(int)
    print(f"\n--- {name} ---")
    print(classification_report(y_test, y_pred, digits=4))
    try:
        roc = roc_auc_score(y_test, y_score)
        precision, recall, _ = precision_recall_curve(y_test, y_score)
        pr_auc = auc(recall, precision)
        print(f"ROC AUC: {roc:.4f} | PR AUC: {pr_auc:.4f}")
    except Exception as e:
        print("Could not compute ROC/PR AUC:", e)

def plot_pr_roc(y_test, scores, label):
    from sklearn.metrics import roc_curve
    fpr, tpr, _ = roc_curve(y_test, scores)
    plt.plot(fpr, tpr, label=f"{label} (AUC={roc_auc_score(y_test, scores):.3f})")

# ---------- EDA MODULE ----------
def perform_eda(df, label_col):
    """Perform basic EDA on dataset."""
    print("\n===== EXPLORATORY DATA ANALYSIS =====")
    print("\nDataset Info:")
    print(df.info())
    print("\nMissing Values:")
    print(df.isnull().sum().sort_values(ascending=False).head(10))
    print("\nClass Distribution:")
    print(df[label_col].value_counts())

    # Correlation heatmap
    plt.figure(figsize=(10, 6))
    corr = df.corr(numeric_only=True)
    sns.heatmap(corr, cmap="coolwarm", center=0)
    plt.title("Correlation Heatmap")
    plt.tight_layout()
    plt.savefig("eda_correlation_heatmap.png")
    plt.close()

    # Class distribution plot
    plt.figure(figsize=(5, 4))
    sns.countplot(x=label_col, data=df)
    plt.title("Class Distribution")
    plt.savefig("eda_class_distribution.png")
    plt.close()

    # Basic statistics
    print("\nDescriptive Statistics:")
    print(df.describe().T.head(10))

# ---------- MAIN PIPELINE ----------
def main():
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"{DATA_PATH} not found. Please place CSV in the same directory.")

    df = pd.read_csv(DATA_PATH)
    print("Dataset loaded successfully.")
    print("Shape:", df.shape)

    label_col = find_label_column(df)
    print("Detected label column:", label_col)

    perform_eda(df, label_col)

    X = df.drop(columns=[label_col]).copy()
    y = df[label_col].astype(int).copy()

    X = preprocess_features(X)

    scaler = StandardScaler()
    for col in ['Time', 'Amount']:
        if col in X.columns:
            X[[col]] = scaler.fit_transform(X[[col]])

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE
    )
    print("Train/Test sizes:", X_train.shape, X_test.shape)

    print("\nBefore SMOTE:", np.bincount(y_train))
    sm = SMOTE(random_state=RANDOM_STATE)
    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
    print("After SMOTE:", np.bincount(y_train_res))

    # ----- Models -----
    lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)
    lr.fit(X_train_res, y_train_res)
    evaluate_model("Logistic Regression", lr, X_test, y_test)

    rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)
    rf.fit(X_train_res, y_train_res)
    evaluate_model("Random Forest", rf, X_test, y_test)

    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE)
    xgb.fit(X_train_res, y_train_res)
    evaluate_model("XGBoost", xgb, X_test, y_test)

    lgbm = lgb.LGBMClassifier(random_state=RANDOM_STATE)
    lgbm.fit(X_train_res, y_train_res)
    evaluate_model("LightGBM", lgbm, X_test, y_test)

    estimators = [('rf', rf), ('xgb', xgb), ('lgbm', lgbm)]
    stack = StackingClassifier(estimators=estimators,
                               final_estimator=LogisticRegression(),
                               cv=5, n_jobs=-1)
    stack.fit(X_train_res, y_train_res)
    evaluate_model("Stacking Ensemble", stack, X_test, y_test)

    # ----- Anomaly Detection -----
    X_train_norm = X_train[y_train == 0]
    iso = IsolationForest(contamination=0.001, random_state=RANDOM_STATE)
    iso.fit(X_train_norm)
    iso_preds = np.where(iso.predict(X_test) == -1, 1, 0)
    print("\nIsolation Forest:")
    print(classification_report(y_test, iso_preds, digits=4))

    try:
        ocsvm = OneClassSVM(kernel='rbf', nu=0.001, gamma='scale')
        ocsvm.fit(X_train_norm)
        ocsvm_preds = np.where(ocsvm.predict(X_test) == -1, 1, 0)
        print("\nOne-Class SVM:")
        print(classification_report(y_test, ocsvm_preds, digits=4))
    except Exception as e:
        print("OneClassSVM skipped:", e)

    # ----- Autoencoder -----
    X_train_norm_arr = X_train_norm.values
    X_test_arr = X_test.values
    input_dim = X_train_norm_arr.shape[1]
    encoding_dim = max(8, input_dim // 2)

    inp = Input(shape=(input_dim,))
    encoded = Dense(encoding_dim, activation='relu')(inp)
    encoded = Dense(encoding_dim // 2, activation='relu')(encoded)
    decoded = Dense(encoding_dim // 2, activation='relu')(encoded)
    decoded = Dense(input_dim, activation='linear')(decoded)

    autoencoder = Model(inputs=inp, outputs=decoded)
    autoencoder.compile(optimizer='adam', loss='mse')
    early = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    autoencoder.fit(X_train_norm_arr, X_train_norm_arr,
                    epochs=50, batch_size=256, validation_split=0.1,
                    callbacks=[early], verbose=1)

    reconstructions = autoencoder.predict(X_test_arr)
    mse = np.mean(np.power(X_test_arr - reconstructions, 2), axis=1)
    recon_train = autoencoder.predict(X_train_norm_arr)
    mse_train = np.mean(np.power(X_train_norm_arr - recon_train, 2), axis=1)
    threshold = np.percentile(mse_train, 99)
    pred_ae = (mse > threshold).astype(int)
    print("\nAutoencoder (threshold=99th percentile):")
    print(f"Threshold: {threshold:.6f}")
    print(classification_report(y_test, pred_ae, digits=4))

    # ----- SHAP Explainability -----
    try:
        sample_idx = np.random.choice(X_test.shape[0], min(200, X_test.shape[0]), replace=False)
        X_test_sample = X_test.iloc[sample_idx]
        explainer = shap.TreeExplainer(xgb)
        shap_values = explainer.shap_values(X_test_sample)
        shap.summary_plot(shap_values, X_test_sample, plot_type="bar", show=False)
        plt.savefig("shap_summary.png", bbox_inches='tight')
        plt.close()
    except Exception as e:
        print("SHAP skipped:", e)

    # ----- Save Models -----
    joblib.dump(rf, "rf_model.joblib")
    joblib.dump(xgb, "xgb_model.joblib")
    joblib.dump(lgbm, "lgbm_model.joblib")
    joblib.dump(stack, "stack_model.joblib")

    # ----- ROC Curves -----
    models = {'LR': lr, 'RF': rf, 'XGB': xgb, 'LGBM': lgbm, 'STACK': stack}
    plt.figure(figsize=(8, 6))
    for name, model in models.items():
        if hasattr(model, "predict_proba"):
            scores = model.predict_proba(X_test)[:, 1]
        elif hasattr(model, "decision_function"):
            scores = model.decision_function(X_test)
        else:
            scores = model.predict(X_test)
        plot_pr_roc(y_test, scores, name)
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curves")
    plt.legend()
    plt.savefig("roc_comparison.png", bbox_inches='tight')
    plt.close()
    print("ROC comparison saved as roc_comparison.png")

if __name__ == "__main__":
    main()
